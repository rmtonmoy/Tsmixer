{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e9f9fe6-d2e8-4a74-87e1-39c12d5079f3",
   "metadata": {},
   "source": [
    "# TSMixer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85894f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pytorch-tsmixer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "52a55469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([401, 14, 12])\n",
      "torch.Size([401, 5, 1])\n",
      "Epoch [1/100000], Loss: 4168795.5000\n",
      "Epoch [2/100000], Loss: 4168856.0000\n",
      "Epoch [3/100000], Loss: 4168802.2500\n",
      "Epoch [4/100000], Loss: 4168820.7500\n",
      "Epoch [5/100000], Loss: 4168730.7500\n",
      "Epoch [6/100000], Loss: 4168756.2500\n",
      "Epoch [7/100000], Loss: 4168692.7500\n",
      "Epoch [8/100000], Loss: 4168633.2500\n",
      "Epoch [9/100000], Loss: 4168600.7500\n",
      "Epoch [10/100000], Loss: 4168500.2500\n",
      "Epoch [11/100000], Loss: 4168541.7500\n",
      "Epoch [12/100000], Loss: 4168418.2500\n",
      "Epoch [13/100000], Loss: 4168280.7500\n",
      "Epoch [14/100000], Loss: 4168336.2500\n",
      "Epoch [15/100000], Loss: 4168222.7500\n",
      "Epoch [16/100000], Loss: 4168098.7500\n",
      "Epoch [17/100000], Loss: 4167901.2500\n",
      "Epoch [18/100000], Loss: 4167960.7500\n",
      "Epoch [19/100000], Loss: 4167585.2500\n",
      "Epoch [20/100000], Loss: 4167586.0000\n",
      "Epoch [21/100000], Loss: 4167400.0000\n",
      "Epoch [22/100000], Loss: 4167344.5000\n",
      "Epoch [23/100000], Loss: 4167079.2500\n",
      "Epoch [24/100000], Loss: 4166797.2500\n",
      "Epoch [25/100000], Loss: 4166606.0000\n",
      "Epoch [26/100000], Loss: 4166530.7500\n",
      "Epoch [27/100000], Loss: 4166415.7500\n",
      "Epoch [28/100000], Loss: 4166143.5000\n",
      "Epoch [29/100000], Loss: 4165888.7500\n",
      "Epoch [30/100000], Loss: 4165469.2500\n",
      "Epoch [31/100000], Loss: 4165029.0000\n",
      "Epoch [32/100000], Loss: 4165045.5000\n",
      "Epoch [33/100000], Loss: 4164885.7500\n",
      "Epoch [34/100000], Loss: 4164241.7500\n",
      "Epoch [35/100000], Loss: 4164031.5000\n",
      "Epoch [36/100000], Loss: 4163560.0000\n",
      "Epoch [37/100000], Loss: 4163408.5000\n",
      "Epoch [38/100000], Loss: 4162868.0000\n",
      "Epoch [39/100000], Loss: 4162569.7500\n",
      "Epoch [40/100000], Loss: 4161920.7500\n",
      "Epoch [41/100000], Loss: 4161771.2500\n",
      "Epoch [42/100000], Loss: 4161063.2500\n",
      "Epoch [43/100000], Loss: 4160599.7500\n",
      "Epoch [44/100000], Loss: 4159913.0000\n",
      "Epoch [45/100000], Loss: 4159299.0000\n",
      "Epoch [46/100000], Loss: 4158777.2500\n",
      "Epoch [47/100000], Loss: 4158689.2500\n",
      "Epoch [48/100000], Loss: 4157746.5000\n",
      "Epoch [49/100000], Loss: 4157398.5000\n",
      "Epoch [50/100000], Loss: 4156627.2500\n",
      "Epoch [51/100000], Loss: 4155961.5000\n",
      "Epoch [52/100000], Loss: 4155027.5000\n",
      "Epoch [53/100000], Loss: 4154840.0000\n",
      "Epoch [54/100000], Loss: 4153909.5000\n",
      "Epoch [55/100000], Loss: 4153926.0000\n",
      "Epoch [56/100000], Loss: 4152763.5000\n",
      "Epoch [57/100000], Loss: 4151851.0000\n",
      "Epoch [58/100000], Loss: 4151532.7500\n",
      "Epoch [59/100000], Loss: 4149509.2500\n",
      "Epoch [60/100000], Loss: 4149932.0000\n",
      "Epoch [61/100000], Loss: 4147974.7500\n",
      "Epoch [62/100000], Loss: 4147118.2500\n",
      "Epoch [63/100000], Loss: 4146768.2500\n",
      "Epoch [64/100000], Loss: 4145696.7500\n",
      "Epoch [65/100000], Loss: 4144182.2500\n",
      "Epoch [66/100000], Loss: 4144103.2500\n",
      "Epoch [67/100000], Loss: 4141754.2500\n",
      "Epoch [68/100000], Loss: 4142324.2500\n",
      "Epoch [69/100000], Loss: 4139431.5000\n",
      "Epoch [70/100000], Loss: 4138716.5000\n",
      "Epoch [71/100000], Loss: 4137399.7500\n",
      "Epoch [72/100000], Loss: 4135726.7500\n",
      "Epoch [73/100000], Loss: 4135158.5000\n",
      "Epoch [74/100000], Loss: 4133621.7500\n",
      "Epoch [75/100000], Loss: 4133165.2500\n",
      "Epoch [76/100000], Loss: 4131007.5000\n",
      "Epoch [77/100000], Loss: 4131126.2500\n",
      "Epoch [78/100000], Loss: 4127209.5000\n",
      "Epoch [79/100000], Loss: 4127514.0000\n",
      "Epoch [80/100000], Loss: 4125314.0000\n",
      "Epoch [81/100000], Loss: 4123437.5000\n",
      "Epoch [82/100000], Loss: 4123063.0000\n",
      "Epoch [83/100000], Loss: 4120028.2500\n",
      "Epoch [84/100000], Loss: 4118907.5000\n",
      "Epoch [85/100000], Loss: 4118484.7500\n",
      "Epoch [86/100000], Loss: 4114821.0000\n",
      "Epoch [87/100000], Loss: 4111916.5000\n",
      "Epoch [88/100000], Loss: 4110389.7500\n",
      "Epoch [89/100000], Loss: 4108697.5000\n",
      "Epoch [90/100000], Loss: 4108099.0000\n",
      "Epoch [91/100000], Loss: 4106725.7500\n",
      "Epoch [92/100000], Loss: 4102607.5000\n",
      "Epoch [93/100000], Loss: 4098505.0000\n",
      "Epoch [94/100000], Loss: 4098277.0000\n",
      "Epoch [95/100000], Loss: 4097259.2500\n",
      "Epoch [96/100000], Loss: 4095954.0000\n",
      "Epoch [97/100000], Loss: 4090454.5000\n",
      "Epoch [98/100000], Loss: 4089636.5000\n",
      "Epoch [99/100000], Loss: 4088288.7500\n",
      "Epoch [100/100000], Loss: 4087917.2500\n",
      "Epoch [101/100000], Loss: 4080530.0000\n",
      "Epoch [102/100000], Loss: 4077265.2500\n",
      "Epoch [103/100000], Loss: 4080734.7500\n",
      "Epoch [104/100000], Loss: 4073204.7500\n",
      "Epoch [105/100000], Loss: 4069541.0000\n",
      "Epoch [106/100000], Loss: 4065830.7500\n",
      "Epoch [107/100000], Loss: 4062761.5000\n",
      "Epoch [108/100000], Loss: 4062138.7500\n",
      "Epoch [109/100000], Loss: 4059338.5000\n",
      "Epoch [110/100000], Loss: 4061376.5000\n",
      "Epoch [111/100000], Loss: 4056114.0000\n",
      "Epoch [112/100000], Loss: 4049335.2500\n",
      "Epoch [113/100000], Loss: 4047590.7500\n",
      "Epoch [114/100000], Loss: 4040069.0000\n",
      "Epoch [115/100000], Loss: 4040500.7500\n",
      "Epoch [116/100000], Loss: 4034548.2500\n",
      "Epoch [117/100000], Loss: 4030403.0000\n",
      "Epoch [118/100000], Loss: 4031976.0000\n",
      "Epoch [119/100000], Loss: 4028151.7500\n",
      "Epoch [120/100000], Loss: 4019924.0000\n",
      "Epoch [121/100000], Loss: 4022516.0000\n",
      "Epoch [122/100000], Loss: 4017399.2500\n",
      "Epoch [123/100000], Loss: 4011076.5000\n",
      "Epoch [124/100000], Loss: 4008598.5000\n",
      "Epoch [125/100000], Loss: 3999134.0000\n",
      "Epoch [126/100000], Loss: 3990368.5000\n",
      "Epoch [127/100000], Loss: 3992907.2500\n",
      "Epoch [128/100000], Loss: 3990137.5000\n",
      "Epoch [129/100000], Loss: 3984536.7500\n",
      "Epoch [130/100000], Loss: 3981780.2500\n",
      "Epoch [131/100000], Loss: 3976287.2500\n",
      "Epoch [132/100000], Loss: 3972824.0000\n",
      "Epoch [133/100000], Loss: 3963809.2500\n",
      "Epoch [134/100000], Loss: 3962281.5000\n",
      "Epoch [135/100000], Loss: 3955703.2500\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 74\u001b[0m\n\u001b[1;32m     71\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, y)\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;66;03m# Backward pass and optimize\u001b[39;00m\n\u001b[0;32m---> 74\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m], Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_env/lib/python3.11/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_env/lib/python3.11/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchtsmixer import TSMixer\n",
    "import pandas as pd \n",
    "import sys\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "\n",
    "ts = pd.read_csv('covidDataWithSentiment.csv')\n",
    "\n",
    "# Drop cumulative datas?\n",
    "cumulative_clmsn = []\n",
    "for clmn_name in ts:\n",
    "    if \"Cumulative\" in clmn_name:\n",
    "        cumulative_clmsn.append(clmn_name)\n",
    "for entry in cumulative_clmsn:\n",
    "    ts.drop(columns=entry, inplace=True)\n",
    "\n",
    "# Drop date\n",
    "data = ts.select_dtypes(include=['float64', 'float32', 'float16', 'int64', 'int32', 'int16', 'int8', 'uint8'])\n",
    "data = data.fillna(0)\n",
    "\n",
    "# Model parameters\n",
    "sequence_length = 14\n",
    "prediction_length = 5\n",
    "input_channels = len(data.columns) - 1\n",
    "output_channels = 1\n",
    "\n",
    "\n",
    "X_train = torch.empty(0, sequence_length, input_channels)\n",
    "y_train = torch.empty(0, prediction_length, output_channels)\n",
    "\n",
    "for start in range(1, len(data) - sequence_length - prediction_length + 1):\n",
    "    train_inst = torch.tensor(data.iloc[start:start + sequence_length, :-1].values, dtype=torch.float32)\n",
    "    label_inst = torch.tensor(data.iloc[start + sequence_length : start + sequence_length + prediction_length, -1:].values, dtype=torch.float32)    \n",
    "\n",
    "    X_train = torch.cat((X_train, train_inst.unsqueeze(0)), dim = 0)\n",
    "    y_train = torch.cat((y_train, label_inst.unsqueeze(0)), dim = 0)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "\n",
    "dataset = TensorDataset(X_train, y_train)\n",
    "# Create a DataLoader\n",
    "batch_size = 32\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "# Create the TSMixer model\n",
    "model = TSMixer(sequence_length, prediction_length, input_channels, output_channels)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100000\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for X, y in dataloader:\n",
    "        # Extract a single batch from the dataset   \n",
    "                \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(X) \n",
    "        loss = criterion(outputs, y)\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "print(\"Training complete\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
